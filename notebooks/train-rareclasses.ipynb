{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 5.863545,
     "end_time": "2020-12-20T18:13:59.855931",
     "exception": false,
     "start_time": "2020-12-20T18:13:53.992386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import sys\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import label_ranking_average_precision_score, accuracy_score\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd \n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "import albumentations as albu\n",
    "from albumentations import pytorch as AT\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import pretrainedmodels\n",
    "from resnest.torch import resnest50\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import src.audio_augs as aa\n",
    "from src.utils import patch_first_conv\n",
    "from src.loss import lsep_loss_stable, lsep_loss\n",
    "from src.batch_mixer import BatchMixer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "papermill": {
     "duration": 0.37611,
     "end_time": "2020-12-20T18:14:00.248664",
     "exception": false,
     "start_time": "2020-12-20T18:13:59.872554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_folder_path = \"../data/train/\"\n",
    "test_folder_path = \"../data/test/\"\n",
    "sample_submission = \"../data/sample_submission.csv\"\n",
    "train_tp_path = \"../data/train_tp.csv\"\n",
    "train_fp_path = \"../data/train_fp.csv\"\n",
    "train_tp_folds = pd.read_csv(\"../data/train_tp_folds_v3.csv\")\n",
    "train_fp_folds = pd.read_csv(\"train_fp_folds.csv\").drop(\"Unnamed: 0\", 1)\n",
    "\n",
    "train_files = os.listdir(train_folder_path)\n",
    "test_files = os.listdir(test_folder_path)\n",
    "\n",
    "train_tp = pd.read_csv(train_tp_path)\n",
    "train_fp = pd.read_csv(train_fp_path)\n",
    "_df = pd.read_csv(\"missing_3classes.csv\")\n",
    "_df = _df.drop(columns=\"Unnamed: 0\")\n",
    "\n",
    "\n",
    "train=pd.concat([train_fp, train_tp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    SEED = 17\n",
    "    NUM_BIRDS = 5\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_WORKERS = 8\n",
    "    IMG_H = 320\n",
    "    IMG_W = 512\n",
    "    \n",
    "    #optimizer params\n",
    "    LR = 0.01\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "    MOMENTUM = 0.9\n",
    "    \n",
    "    #scheduler params\n",
    "    FACTOR = 0.4\n",
    "    PATIENCE = 3\n",
    "    \n",
    "    #spec params\n",
    "    FFT = 2048\n",
    "    HOP = 512\n",
    "    MEL = 224\n",
    "    SR = 48000\n",
    "    Fmin = 84 \n",
    "    Fmax = 15056\n",
    "    LENGTH_1  = 6 * SR\n",
    "    LENGTH_2 = 3*SR\n",
    "    #TODO: MAKE AUGS CONF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.030754,
     "end_time": "2020-12-20T18:14:00.693737",
     "exception": false,
     "start_time": "2020-12-20T18:14:00.662983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(Config.SEED)    \n",
    "\n",
    "def wav2mel(wav):\n",
    "\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(wav, n_fft=Config.FFT, \n",
    "                                              hop_length=Config.HOP, \n",
    "                                              sr=Config.SR, \n",
    "                                              fmin=Config.Fmin, \n",
    "                                              fmax=Config.Fmax, \n",
    "                                              power=2, n_mels=Config.MEL)\n",
    "    mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "\n",
    "    return mel_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.03189,
     "end_time": "2020-12-20T18:14:00.743201",
     "exception": false,
     "start_time": "2020-12-20T18:14:00.711311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RainforestDataset(Dataset):\n",
    "    def __init__(self, df, audio_transforms = None, image_transforms = None,):\n",
    "        self.audio_transforms = audio_transforms\n",
    "        self.img_transforms = image_transforms\n",
    "        self.df = df\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    \n",
    "    def wav2mel(self, wav):\n",
    "        \n",
    "        \n",
    "        mel_spec = librosa.feature.melspectrogram(wav, n_fft=Config.FFT, \n",
    "                                                  hop_length=Config.HOP, \n",
    "                                                  sr=Config.SR, \n",
    "                                                  fmin=Config.Fmin, \n",
    "                                                  fmax=Config.Fmax, \n",
    "                                                  power=2, n_mels=Config.MEL)\n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        \n",
    "        return mel_spec\n",
    "    \n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sample = copy.deepcopy(self.df.iloc[idx, :].values)\n",
    "\n",
    "        bird_id = sample[1]\n",
    "        #true_or_not = sample[-1]\n",
    "        label_array = np.zeros(Config.NUM_BIRDS, dtype=np.single)\n",
    "        labels_dict = {3 : 0, 18: 1, 12: 2, 7 : 3, 15: 4}\n",
    "\n",
    "        label_array[labels_dict[bird_id]] = 1.\n",
    "        \n",
    "        wav, sr = librosa.load(train_folder_path + sample[0] + \".flac\", sr=None)\n",
    "        \n",
    "        tmin = float(sample[3]) * sr\n",
    "        tmax = float(sample[5]) * sr\n",
    "        center = np.round((tmin + tmax) / 2)\n",
    "        \n",
    "        check_length = True\n",
    "        \n",
    "        if check_length: #sample[11] == 1 or sample[11] == 2:\n",
    "            length = Config.LENGTH_2\n",
    "        else:\n",
    "            length = Config.LENGTH_1\n",
    "        \n",
    "        beginning = center - length / 2\n",
    "        if beginning < 0:\n",
    "            beginning = 0\n",
    "            \n",
    "        ending = beginning + length\n",
    "        if ending > len(wav):\n",
    "            ending = len(wav)\n",
    "            beginning = ending - length \n",
    "            \n",
    "        wav_slice = wav[int(beginning):int(ending)]\n",
    "        \n",
    "        if self.audio_transforms: # and bird_id not in (3, 7, 8, 9):\n",
    "            wav_slice =  self.audio_transforms(wav_slice)\n",
    "        \n",
    "        mel_spec = self.wav2mel(wav_slice)\n",
    "        mel_spec = np.expand_dims(mel_spec, axis=2).astype(np.float32)\n",
    "        #print(np.max(mel_spec))\n",
    "        \n",
    "        if self.img_transforms:\n",
    "            image =  self.img_transforms(image=mel_spec)\n",
    "            mel_spec = image[\"image\"]\n",
    "            \n",
    "            \n",
    "        \n",
    "        return mel_spec / 100.0, label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 0.190203,
     "end_time": "2020-12-20T18:14:00.950678",
     "exception": false,
     "start_time": "2020-12-20T18:14:00.760475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 332 examples\n",
      "Validating on 44 examples\n"
     ]
    }
   ],
   "source": [
    "train_tp_folds[\"true\"] = 1\n",
    "train_fp_folds[\"true\"] = 0\n",
    "\n",
    "problem_ids = [3,18,12,7,15]\n",
    "for i in problem_ids:\n",
    "    if i == 3:\n",
    "        rare_train = train_tp_folds[(train_tp_folds.species_id == i)]\n",
    "    else:\n",
    "        rare_train = rare_train.append(train_tp_folds[(train_tp_folds.species_id == i)], ignore_index=False )  \n",
    "\n",
    "fold = 1\n",
    "X_train = train_tp_folds[(train_tp_folds['fold'] != fold) & (train_tp_folds['fold'] != 5)].reset_index(drop=True)\n",
    "X_val = train_tp_folds[train_tp_folds['fold'] == fold].reset_index(drop=True)\n",
    "X_test = train_tp_folds[train_tp_folds['fold'] == 5].reset_index(drop=True)\n",
    "\n",
    "X_train_rare = rare_train[(rare_train['fold'] != fold)].reset_index(drop=True)\n",
    "X_val_rare = rare_train[rare_train['fold'] == fold].reset_index(drop=True)\n",
    "_df = _df[_df.recording_id.isin(X_train_rare.recording_id)]\n",
    "X_train_rare = X_train_rare[_df.columns]\n",
    "X_train_rare = pd.concat([X_train_rare, _df])\n",
    "\n",
    "\n",
    "print('Training on ' + str(len(X_train_rare)) + ' examples')\n",
    "print('Validating on ' + str(len(X_val_rare)) + ' examples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.031559,
     "end_time": "2020-12-20T18:14:01.049055",
     "exception": false,
     "start_time": "2020-12-20T18:14:01.017496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_transform_train = aa.Compose([\n",
    "  aa.OneOf([\n",
    "    aa.GaussianNoiseSNR(min_snr=5),\n",
    "    aa.PinkNoiseSNR(min_snr=5)\n",
    "  ]),\n",
    "  aa.PitchShift(max_steps=2, sr=Config.SR, p=0.5),\n",
    "  #TimeStretch(max_rate=1.2, p=0.1),\n",
    "  aa.TimeShift(sr=Config.SR),\n",
    "  aa.VolumeControl(mode=\"sine\", p=0.5)\n",
    "])\n",
    "\n",
    "\n",
    "image_transform_train = albu.Compose([\n",
    "        albu.Resize(Config.IMG_H, Config.IMG_W),\n",
    "        #albu.IAAAdditiveGaussianNoise(p=0.2),\n",
    "        #albu.Cutout(num_holes=8, max_h_size=16, max_w_size=16, p=0.25),\n",
    "        #albu.OneOf(\n",
    "        #    [\n",
    "        #        albu.IAASharpen(p=1),\n",
    "        #        albu.Blur(blur_limit=3, p=1),\n",
    "        #        albu.MotionBlur(blur_limit=3, p=1),\n",
    "        #    ],\n",
    "        #    p=0.3,\n",
    "        #),\n",
    "\n",
    "        #albu.Normalize((-48.01), (8.45), max_pixel_value=1),\n",
    "        AT.ToTensorV2()\n",
    "    ])\n",
    "\n",
    "\n",
    "transform_val = albu.Compose([\n",
    "        albu.Resize(Config.IMG_H, Config.IMG_W),\n",
    "        #albu.Normalize((-48.01), (8.45), max_pixel_value=1),\n",
    "        AT.ToTensorV2()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 10.752926,
     "end_time": "2020-12-20T18:14:11.820042",
     "exception": false,
     "start_time": "2020-12-20T18:14:01.067116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = RainforestDataset(X_train_rare, audio_transforms=audio_transform_train, image_transforms=image_transform_train)\n",
    "val_dataset = RainforestDataset(X_val_rare, audio_transforms=None, image_transforms=transform_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers = Config.NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, shuffle = False, num_workers = Config.NUM_WORKERS)\n",
    "\n",
    "#model = pretrainedmodels.__dict__['se_resnext50_32x4d']( pretrained= \"imagenet\")\n",
    "#model.load_state_dict(torch.load(\"../input/seresnext50/se_resnext50_32x4d-a260b3a4.pth\"))\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "#model = resnest50(pretrained=True)\n",
    "model.avg_pool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(512, 512),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, Config.NUM_BIRDS)\n",
    ")\n",
    "\n",
    "\n",
    "patch_first_conv(model, 1)\n",
    "# Picked for this notebook; pick new ones after major changes (such as adding train_fp to train data)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, )# momentum = 0.9)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=Config.LR, weight_decay=Config.WEIGHT_DECAY, momentum=Config.MOMENTUM)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = Config.PATIENCE, gamma = Config.FACTOR)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = Config.PATIENCE, factor = Config.FACTOR, mode = \"max\")\n",
    "scheduler =torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15)\n",
    "\n",
    "mixer = BatchMixer(p=0.45)\n",
    "# This loss function is not exactly suited for competition metric, which only cares about ranking of predictions\n",
    "# Exploring different loss fuctions would be a good idea\n",
    "#pos_weights = torch.ones(Config.NUM_BIRDS)\n",
    "#pos_weights = pos_weights * Config.NUM_BIRDS\n",
    "#loss_function = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    #loss_function = loss_function.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_val_file(record_id, df):\n",
    "    wav, sr = librosa.load('../data/train/' + record_id + \".flac\", sr=None)\n",
    "\n",
    "        \n",
    "\n",
    "    # Split for enough segments to not miss anything\n",
    "    segments = len(wav) / Config.LENGTH_2\n",
    "    segments = int(np.ceil(segments))\n",
    "    mel_array = []\n",
    "    for i in range(0, segments):\n",
    "        if (i + 1) * Config.LENGTH_2 > len(wav):\n",
    "            slice = wav[len(wav) - Config.LENGTH_2:len(wav)]\n",
    "        else:\n",
    "            slice = wav[i * Config.LENGTH_1:(i + 1) * Config.LENGTH_2]\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(slice, n_fft=Config.FFT, \n",
    "                                                  hop_length=Config.HOP, \n",
    "                                                  sr=Config.SR, \n",
    "                                                  fmin=Config.Fmin, \n",
    "                                                  fmax=Config.Fmax, \n",
    "                                                  power=2, n_mels=Config.MEL)\n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        transform_val = albu.Compose([albu.Resize(Config.IMG_H, Config.IMG_W),]) \n",
    "        mel_spec = np.expand_dims(mel_spec, axis=2).astype(np.float32) / 100.0\n",
    "        augmented = transform_val(image = mel_spec)\n",
    "        mel_spec = augmented[ \"image\"]\n",
    "        mel_array.append(mel_spec)\n",
    "        \n",
    "    label_array = np.zeros(Config.NUM_BIRDS, dtype=np.single)\n",
    "    species_ids = df[(df.recording_id==record_id)].species_id.unique()\n",
    "    label_array[species_ids] = 1.\n",
    "    \n",
    "\n",
    "  \n",
    "    return np.array(mel_array), label_array\n",
    "\n",
    "def lwlrap(truth, scores):\n",
    "    \"\"\"Calculate the overall lwlrap using sklearn.metrics.lrap.\"\"\"\n",
    "    # sklearn doesn't correctly apply weighting to samples with no labels, so just skip them.\n",
    "    sample_weight = np.sum(truth > 0, axis=1)\n",
    "    nonzero_weight_sample_indices = np.flatnonzero(sample_weight > 0)\n",
    "    overall_lwlrap = label_ranking_average_precision_score(\n",
    "      truth[nonzero_weight_sample_indices, :] > 0,\n",
    "      scores[nonzero_weight_sample_indices, :],\n",
    "      sample_weight=sample_weight[nonzero_weight_sample_indices])\n",
    "    return overall_lwlrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "papermill": {
     "duration": 800.956277,
     "end_time": "2020-12-20T18:27:32.796472",
     "exception": false,
     "start_time": "2020-12-20T18:14:11.840195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:23,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training end. LR: 0.0003, Loss: 1.5213377589271182, Correct answers: 215/332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 validation end. LR: 0.0003, Loss: 1.7929445107777913, Accuracy: 9/44, Val metric: 0.49085648148148153\n",
      "Saving new best model at epoch 0 (0.028645833488553762/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:24,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training end. LR: 0.0002967221401100708, Loss: 1.2958367523692904, Correct answers: 235/332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 validation end. LR: 0.0002967221401100708, Loss: 1.675284465154012, Accuracy: 23/44, Val metric: 0.6836805555555555\n",
      "Saving new best model at epoch 1 (0.0703125/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:24,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 training end. LR: 0.0002870318186463901, Loss: 1.1456916474160694, Correct answers: 244/332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 validation end. LR: 0.0002870318186463901, Loss: 0.8493809898694357, Accuracy: 28/44, Val metric: 0.8027777777777777\n",
      "Saving new best model at epoch 2 (0.1067708358168602/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:24,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 training end. LR: 0.0002713525491562421, Loss: 1.1468930698576427, Correct answers: 238/332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 validation end. LR: 0.0002713525491562421, Loss: 0.7201937834421793, Accuracy: 28/44, Val metric: 0.8159722222222222\n",
      "Saving new best model at epoch 3 (0.1067708358168602/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:24,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 training end. LR: 0.00025036959095382875, Loss: 1.0497864541553317, Correct answers: 237/332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 validation end. LR: 0.00025036959095382875, Loss: 0.7132663801312447, Accuracy: 34/44, Val metric: 0.8746527777777778\n",
      "Saving new best model at epoch 4 (0.1041666716337204/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:25,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 training end. LR: 0.00022500000000000002, Loss: 1.0033749654179527, Correct answers: 253/332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 validation end. LR: 0.00022500000000000002, Loss: 0.8138600389162699, Accuracy: 27/44, Val metric: 0.7993055555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:22,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 training end. LR: 0.00019635254915624213, Loss: 0.9374407927195231, Correct answers: 261/332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 validation end. LR: 0.00019635254915624213, Loss: 0.7075935502847036, Accuracy: 29/44, Val metric: 0.8236111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:21,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 training end. LR: 0.00016567926949014806, Loss: 1.00148359082994, Correct answers: 258/332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 validation end. LR: 0.00016567926949014806, Loss: 0.6496172696352005, Accuracy: 31/44, Val metric: 0.8559027777777777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:21,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 training end. LR: 0.00013432073050985205, Loss: 0.988366234870184, Correct answers: 255/332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 validation end. LR: 0.00013432073050985205, Loss: 0.9737612158060074, Accuracy: 25/44, Val metric: 0.7680555555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:21,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 training end. LR: 0.00010364745084375793, Loss: 0.8743262404487246, Correct answers: 266/332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 validation end. LR: 0.00010364745084375793, Loss: 0.6451925933361053, Accuracy: 31/44, Val metric: 0.859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:22,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 training end. LR: 7.500000000000006e-05, Loss: 0.7987490211214338, Correct answers: 260/332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 validation end. LR: 7.500000000000006e-05, Loss: 0.5083025644222895, Accuracy: 38/44, Val metric: 0.9322916666666666\n",
      "Saving new best model at epoch 10 (0.109375/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:22,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 training end. LR: 4.963040904617133e-05, Loss: 0.8427585462729136, Correct answers: 264/332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 validation end. LR: 4.963040904617133e-05, Loss: 0.6606930096944174, Accuracy: 33/44, Val metric: 0.8697916666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:21,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 training end. LR: 2.8647450843757904e-05, Loss: 0.8309630680651892, Correct answers: 268/332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 validation end. LR: 2.8647450843757904e-05, Loss: 0.5474882225195566, Accuracy: 37/44, Val metric: 0.9236111111111112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:21,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 training end. LR: 1.2968181353609857e-05, Loss: 0.8986740594818479, Correct answers: 266/332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 validation end. LR: 1.2968181353609857e-05, Loss: 0.5064829091231028, Accuracy: 38/44, Val metric: 0.9340277777777778\n",
      "Saving new best model at epoch 13 (0.109375/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:22,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 training end. LR: 3.2778598899291478e-06, Loss: 0.796790443715595, Correct answers: 278/332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 validation end. LR: 3.2778598899291478e-06, Loss: 0.48622657855351764, Accuracy: 37/44, Val metric: 0.9236111111111112\n"
     ]
    }
   ],
   "source": [
    "best_corrects = 0\n",
    "\n",
    "# Train loop\n",
    "print('Starting training loop')\n",
    "for e in range(0, 15):\n",
    "    # Stats\n",
    "    accumulation_steps  = 2\n",
    "    train_loss = []\n",
    "    train_corr = []\n",
    "    \n",
    "    # Single epoch - train\n",
    "    model.train()\n",
    "    for batch, (data, target) in tqdm(enumerate(train_loader)):\n",
    "        data = data.float()\n",
    "        data, target = mixer(data, target)\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        #loss = loss_function(output, target)\n",
    "        label_smoothing_list = [0.001, 0.0011, 0.0012, 0.00125, 0.001, 0.0011, 0.001, 0.0012]\n",
    "        label_smoothing = random.choice(label_smoothing_list) \n",
    "        targets_smooth = target * (1 - label_smoothing) + 0.5 * label_smoothing\n",
    "        loss = lsep_loss(output, targets_smooth)\n",
    "        \n",
    "        #oss = loss / accumulation_steps                # Normalize our loss (if averaged)\n",
    "        #loss.backward()                                 # Backward pass\n",
    "        #if (batch+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "        #    optimizer.step()                            # Now we can do an optimizer step\n",
    "            #model.zero_grad()                           # Reset gradients tensors\n",
    "  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        \n",
    "        # Stats\n",
    "        vals, answers = torch.max(output, 1)\n",
    "        vals, targets = torch.max(target, 1)\n",
    "        corrects = 0\n",
    "        for i in range(0, len(answers)):\n",
    "            if answers[i] == targets[i]:\n",
    "                corrects = corrects + 1\n",
    "        train_corr.append(corrects)\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "    \n",
    "    # Stats\n",
    "    for g in optimizer.param_groups:\n",
    "        lr = g['lr']\n",
    "    print('Epoch ' + str(e) + ' training end. LR: ' + str(lr) + ', Loss: ' + str(sum(train_loss) / len(train_loss)) +\n",
    "          ', Correct answers: ' + str(sum(train_corr)) + '/' + str(train_dataset.__len__()))\n",
    "    #torch.save(model.state_dict(), 'best_model.pt')\n",
    "    # Single epoch - validation\n",
    "    with torch.no_grad():\n",
    "        # Stats\n",
    "        val_loss = []\n",
    "        val_corr = []\n",
    "        val_metrics = []\n",
    "        acc_metrics = []\n",
    "        answers_list = []\n",
    "        targets_list = []\n",
    "        \n",
    "        model.eval()\n",
    "        for batch, (data, target) in enumerate(val_loader):\n",
    "            data = data.float()\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            output = model(data)\n",
    "            #loss = loss_function(output, target)\n",
    "            loss = lsep_loss(output, target)\n",
    "\n",
    "            \n",
    "            # Stats\n",
    "            th_output = torch.sigmoid(output)  \n",
    "            th_output[th_output >= 0.5] = 1\n",
    "            accuracy = (th_output == target).sum()/(16*24)\n",
    "            \n",
    "            val_metric =  lwlrap( target.cpu().numpy(), output.cpu().numpy())\n",
    "            vals, answers = torch.max(output, 1)\n",
    "            vals, targets = torch.max(target, 1)\n",
    "            answers_list.append(answers)\n",
    "            targets_list.append(targets)\n",
    "            val_metrics.append(val_metric.item())\n",
    "            acc_metrics.append(accuracy.item())\n",
    "            \n",
    "            corrects = 0\n",
    "            for i in range(0, len(answers)):\n",
    "                if answers[i] == targets[i]:\n",
    "                    corrects = corrects + 1\n",
    "            val_corr.append(corrects)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    \n",
    "    valid_epoch_metric = sum(val_metrics) / len(val_loss)\n",
    "    # Stats\n",
    "    print('Epoch ' + str(e) + ' validation end. LR: ' + str(lr) + ', Loss: ' + str(sum(val_loss) / len(val_loss)) +\n",
    "          ', Accuracy: ' + str(sum(val_corr)) + '/' + str(val_dataset.__len__()) + \", Val metric: \" + str(valid_epoch_metric))\n",
    "    \n",
    "    # If this epoch is better than previous on validation, save model\n",
    "    # Validation loss is the more common metric, but in this case our loss is misaligned with competition metric, making accuracy a better metric\n",
    "    if valid_epoch_metric > best_corrects:\n",
    "        print('Saving new best model at epoch ' + str(e) + ' (' + str(sum(acc_metrics)) + '/' + str(len(val_loss)) + ')')\n",
    "        torch.save(model.state_dict(), 'best_model_rare.pt')\n",
    "        best_corrects = valid_epoch_metric\n",
    "        \n",
    "    # Call every epoch\n",
    "    #scheduler.step(valid_epoch_metric)\n",
    "    scheduler.step()\n",
    "\n",
    "# Free memory\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([0, 0, 0, 0, 2, 0, 0, 2, 0, 1, 0, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       "  tensor([1, 1, 0, 2, 1, 2, 2, 2, 0, 0, 2, 3, 3, 4, 3, 3], device='cuda:0'),\n",
       "  tensor([3, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4, 0, 4], device='cuda:0')],\n",
       " [tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       "  tensor([1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3], device='cuda:0'),\n",
       "  tensor([3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_list, targets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "papermill": {
     "duration": 0.995151,
     "end_time": "2020-12-20T18:27:34.774887",
     "exception": false,
     "start_time": "2020-12-20T18:27:33.779736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Already defined above; for reference\n",
    "\n",
    "fft = 2048\n",
    "hop = 512 * 1\n",
    "# Less rounding errors this way\n",
    "sr = 48000\n",
    "length = 3 * sr\n",
    "fmin = 84\n",
    "fmax = 15056\n",
    "\n",
    "def load_test_file(f): \n",
    "    wav, sr = librosa.load('../data/test/' + f, sr=None)\n",
    "\n",
    "    # Split for enough segments to not miss anything\n",
    "    segments = len(wav) / length\n",
    "    segments = int(np.ceil(segments))\n",
    "    \n",
    "    mel_array = []\n",
    "    \n",
    "    for i in range(0, segments):\n",
    "        # Last segment going from the end\n",
    "        if (i + 1) * length > len(wav):\n",
    "            slice = wav[len(wav) - length:len(wav)]\n",
    "        else:\n",
    "            slice = wav[i * length:(i + 1) * length]\n",
    "        \n",
    "        # Same mel spectrogram as before\n",
    "        mel_spec = librosa.feature.melspectrogram(slice, n_fft=fft, hop_length=hop, sr=sr, fmin=fmin, fmax=fmax, power=2, n_mels=Config.MEL)\n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        transform_val = albu.Compose([albu.Resize(Config.IMG_H, Config.IMG_W),]) \n",
    "        #mel_spec = resize(mel_spec, (224, 400))\n",
    "        #mel_spec = mel_spec - np.min(mel_spec)\n",
    "        #mel_spec = mel_spec / np.max(mel_spec)\n",
    "\n",
    "        mel_spec = np.expand_dims(mel_spec, axis=2).astype(np.float32) / 100.\n",
    "        augmented = transform_val(image = mel_spec)\n",
    "        mel_spec = augmented[ \"image\"]\n",
    "        \n",
    "        #mel_spec = np.stack((mel_spec, mel_spec, mel_spec))\n",
    "\n",
    "        mel_array.append(mel_spec)\n",
    "        \n",
    "    \n",
    "    return np.array(mel_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction loop\n",
      "1992\n",
      "Predicted for 100 of 1993 files\n",
      "Predicted for 200 of 1993 files\n",
      "Predicted for 300 of 1993 files\n",
      "Predicted for 400 of 1993 files\n",
      "Predicted for 500 of 1993 files\n",
      "Predicted for 600 of 1993 files\n",
      "Predicted for 700 of 1993 files\n",
      "Predicted for 800 of 1993 files\n",
      "Predicted for 900 of 1993 files\n",
      "Predicted for 1000 of 1993 files\n",
      "Predicted for 1100 of 1993 files\n",
      "Predicted for 1200 of 1993 files\n",
      "Predicted for 1300 of 1993 files\n",
      "Predicted for 1400 of 1993 files\n",
      "Predicted for 1500 of 1993 files\n",
      "Predicted for 1600 of 1993 files\n",
      "Predicted for 1700 of 1993 files\n",
      "Predicted for 1800 of 1993 files\n",
      "Predicted for 1900 of 1993 files\n",
      "Submission generated\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "#model = resnest50(pretrained=True)\n",
    "model.avg_pool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(512, 512),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, Config.NUM_BIRDS)\n",
    ")\n",
    "\n",
    "patch_first_conv(model, 1)\n",
    "\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(f\"best_model_rare.pt\"))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "# Prediction loop\n",
    "print('Starting prediction loop')\n",
    "with open(f'submission_rare_{fold}_fold.csv', 'w', newline='') as csvfile:\n",
    "    submission_writer = csv.writer(csvfile, delimiter=',')\n",
    "    submission_writer.writerow(['recording_id','s0','s1','s2','s3','s4','s5','s6','s7','s8','s9','s10','s11',\n",
    "                               's12','s13','s14','s15','s16','s17','s18','s19','s20','s21','s22','s23'])\n",
    "\n",
    "    test_files = os.listdir('../data/test/')\n",
    "    print(len(test_files))\n",
    "\n",
    "    # Every test file is split on several chunks and prediction is made for each chunk\n",
    "    for i in range(0, len(test_files)):\n",
    "        data = load_test_file(test_files[i]).transpose(0,3,1,2)\n",
    "        data = torch.tensor(data)\n",
    "        data = data.float()\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        # Taking max prediction from all slices per bird species\n",
    "        # Usually you want Sigmoid layer here to convert output to probabilities\n",
    "        # In this competition only relative ranking matters, and not the exact value of prediction, so we can use it directly\n",
    "        maxed_output = torch.max(output, dim=0)[0]\n",
    "        maxed_output = maxed_output.cpu().detach()\n",
    "\n",
    "        file_id = str.split(test_files[i], '.')[0]\n",
    "        write_array = [file_id]\n",
    "\n",
    "        for out in maxed_output:\n",
    "            write_array.append(out.item())\n",
    "\n",
    "        submission_writer.writerow(write_array)\n",
    "\n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print('Predicted for ' + str(i) + ' of ' + str(len(test_files) + 1) + ' files')\n",
    "\n",
    "print('Submission generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "papermill": {
   "duration": 1833.679699,
   "end_time": "2020-12-20T18:44:03.770668",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-20T18:13:30.090969",
   "version": "2.1.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0f15442e94a44248b50672c2777030c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4c435759345342aebeb8f5670af57f75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e56ad2e6997c43d695706f19b3c85f92",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_0f15442e94a44248b50672c2777030c9",
       "value": " 105M/105M [00:05&lt;00:00, 22.0MB/s]"
      }
     },
     "8b8ab01ab9604583ae4dbf53f3058ecf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e0f16c2b57ec45a0a100f1e728ab3d50",
       "max": 110273258,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_98b001f4de7143579e81c44054c3e04c",
       "value": 110273258
      }
     },
     "98b001f4de7143579e81c44054c3e04c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "c71fd161194b4e9d8f6df0c22ddc66c2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e0f16c2b57ec45a0a100f1e728ab3d50": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e56ad2e6997c43d695706f19b3c85f92": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ff6567e773d44b348f8b0ef7a4cf61e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8b8ab01ab9604583ae4dbf53f3058ecf",
        "IPY_MODEL_4c435759345342aebeb8f5670af57f75"
       ],
       "layout": "IPY_MODEL_c71fd161194b4e9d8f6df0c22ddc66c2"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
