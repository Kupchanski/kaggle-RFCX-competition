{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 5.863545,
     "end_time": "2020-12-20T18:13:59.855931",
     "exception": false,
     "start_time": "2020-12-20T18:13:53.992386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import sys\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from typing import Union, List, Dict, Any, cast\n",
    "import random\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import label_ranking_average_precision_score, accuracy_score\n",
    "import torchvision\n",
    "\n",
    "import audiomentations as audioaa\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd \n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "import albumentations as albu\n",
    "from albumentations import pytorch as AT\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "import pretrainedmodels\n",
    "#from resnest.torch import resnest50\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import src.audio_augs as aa\n",
    "from src.utils import patch_first_conv\n",
    "from src.loss import lsep_loss_stable, lsep_loss\n",
    "from src.batch_mixer import BatchMixer\n",
    "from src.pann import *\n",
    "\n",
    "import timm\n",
    "from timm.models.efficientnet import tf_efficientnet_b0_ns, tf_efficientnet_lite4, mobilenetv2_140\n",
    "from torchlibrosa.stft import Spectrogram, LogmelFilterBank\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "papermill": {
     "duration": 0.37611,
     "end_time": "2020-12-20T18:14:00.248664",
     "exception": false,
     "start_time": "2020-12-20T18:13:59.872554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_folder_path = \"../data/train/\"\n",
    "train_np_folder_path = \"../data/train_np/\"\n",
    "test_folder_path = \"../data/test/\"\n",
    "sample_submission = \"../data/sample_submission.csv\"\n",
    "train_tp_path = \"../data/train_tp.csv\"\n",
    "train_fp_path = \"../data/train_fp.csv\"\n",
    "train_tp_folds = pd.read_csv(\"../data/train_tp_folds_v3.csv\")\n",
    "train_fp_folds = pd.read_csv(\"train_fp_folds.csv\").drop(\"Unnamed: 0\", 1)\n",
    "\n",
    "train_files = os.listdir(train_folder_path)\n",
    "test_files = os.listdir(test_folder_path)\n",
    "\n",
    "train_tp = pd.read_csv(train_tp_path)\n",
    "train_fp = pd.read_csv(train_fp_path)\n",
    "\n",
    "_df = pd.read_csv(\"missing_3classes_extended.csv\")\n",
    "_df = _df.drop(columns=\"Unnamed: 0\")\n",
    "\n",
    "pseudo = pd.read_csv(\"pseudolabels_raw.csv\")\n",
    "pseudo_clear = pseudo[pseudo.mean_confidence > 0.99]\n",
    "pseudo_clear = pseudo_clear.rename(columns={\"offset\": \"t_max\", \"onset\": \"t_min\", \"file_id\": \"recording_id\", \"max_confidence\":\"f_min\", \"mean_confidence\": \"f_max\"})\n",
    "pseudo_clear[\"songtype_id\"] =1 \n",
    "pseudo_clear[\"label\"] = 0.9\n",
    "pseudo_clear = pseudo_clear[['recording_id', 'species_id', 'songtype_id', 't_min', 'f_min', 't_max', \"f_max\", \"label\"]]\n",
    "pseudo_clear = pseudo_clear[~(pseudo_clear.species_id == 12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    SEED = 25\n",
    "    NUM_BIRDS = 24\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_WORKERS = 4\n",
    "    FOLD = 0\n",
    "    TEST_FOLD = 5\n",
    "    EPOCHS = 50\n",
    "    \n",
    "    #optimizer params\n",
    "    LR = 0.01\n",
    "    LR_ADAM = 1e-3\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "    MOMENTUM = 0.9\n",
    "    T_MAX = 8\n",
    "    \n",
    "    #scheduler params\n",
    "    FACTOR = 0.8\n",
    "    PATIENCE = 4\n",
    "\n",
    "    SR = 48000\n",
    "    LENGTH_1  = 10* SR\n",
    "    LENGTH_2 = 5 * SR\n",
    "    #TODO: MAKE AUGS CONF\n",
    "    \n",
    "encoder_params = {\n",
    "    \"efficientnet_b0\": {\n",
    "        #\"features\": 1280,\n",
    "        \"features\": 1792,\n",
    "        \"init_op\": partial(mobilenetv2_140, pretrained=True, drop_path_rate=0.2)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "model_param = {\n",
    "        'encoder' : 'efficientnet_b0',\n",
    "        'sample_rate': 48000,\n",
    "        'window_size' : 2048, #* 2, # 512 * 2\n",
    "        'hop_size' : 512, #345 * 2, # 320\n",
    "        'mel_bins' : 224, # 60\n",
    "        'fmin' : 300,\n",
    "        'fmax' : 15000,\n",
    "        'classes_num' : 24\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 0.030754,
     "end_time": "2020-12-20T18:14:00.693737",
     "exception": false,
     "start_time": "2020-12-20T18:14:00.662983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(Config.SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioSEDModel(nn.Module):\n",
    "    def __init__(self, encoder, sample_rate, window_size, hop_size, mel_bins, fmin, fmax, classes_num):\n",
    "        super().__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "        self.interpolate_ratio = 30  # Downsampled ratio\n",
    "        self.mixup_coff = Mixup(1.)\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "        \n",
    "        # Model Encoder\n",
    "        self.encoder = encoder_params[encoder][\"init_op\"]()\n",
    "        self.fc1 = nn.Linear(encoder_params[encoder][\"features\"], 1024, bias=True)\n",
    "        self.att_block = AttBlock(1024, classes_num, activation=\"sigmoid\")\n",
    "        self.bn0 = nn.BatchNorm2d(mel_bins)\n",
    "        self.init_weight()\n",
    "    \n",
    "    def init_weight(self):\n",
    "        init_layer(self.fc1)\n",
    "        init_bn(self.bn0)\n",
    "    \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"Input : (batch_size, data_length)\"\"\"\n",
    "\n",
    "        x = self.spectrogram_extractor(input)\n",
    "        # batch_size x 1 x time_steps x freq_bins\n",
    "        x = self.logmel_extractor(x)\n",
    "        # batch_size x 1 x time_steps x mel_bins\n",
    "\n",
    "        frames_num = x.shape[2]\n",
    "\n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        #print(x.shape)\n",
    "\n",
    "        if self.training and False:\n",
    "            x = self.spec_augmenter(x)\n",
    "        \n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "        \n",
    "        # Output shape (batch size, channels, time, frequency)\n",
    "        x = x.expand(x.shape[0], 3, x.shape[2], x.shape[3])\n",
    "        #print(x.shape)\n",
    "        x = self.encoder.forward_features(x)\n",
    "        #print(x.shape)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        #print(x.shape)\n",
    "\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "        #print(x.shape)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        #print(x.shape)\n",
    "\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n",
    "        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        # Get framewise output\n",
    "        framewise_output = interpolate(segmentwise_output,\n",
    "                                       self.interpolate_ratio)\n",
    "        framewise_output = pad_framewise_output(framewise_output, frames_num)\n",
    "        \n",
    "        framewise_logit = interpolate(segmentwise_logit, self.interpolate_ratio)\n",
    "        framewise_logit = pad_framewise_output(framewise_logit, frames_num)\n",
    "\n",
    "        output_dict = {\n",
    "            \"framewise_output\": framewise_output,\n",
    "            \"segmentwise_output\": segmentwise_output,\n",
    "            \"logit\": logit,\n",
    "            \"framewise_logit\": framewise_logit,\n",
    "            \"clipwise_output\": clipwise_output\n",
    "        }\n",
    "\n",
    "\n",
    "        return output_dict\n",
    "    \n",
    "def crop_or_pad(y, is_train=True):\n",
    "    length = Config.LENGTH_2\n",
    "    if len(y) < length:\n",
    "        \n",
    "        pad_width = length - len(y)\n",
    "        pad_sub = start = np.random.randint(0, pad_width)\n",
    "        \n",
    "        y = np.pad(y, (pad_sub, pad_width-pad_sub), \"minimum\")\n",
    "    elif len(y) > length:\n",
    "        start = np.random.randint(len(y) - length)\n",
    "        \n",
    "        y = y[start:start + length]\n",
    "\n",
    "    y = y.astype(np.float32, copy=False)\n",
    "    #print(y.shape)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "papermill": {
     "duration": 0.03189,
     "end_time": "2020-12-20T18:14:00.743201",
     "exception": false,
     "start_time": "2020-12-20T18:14:00.711311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RainforestDataset(Dataset):\n",
    "    def __init__(self, df, audio_transforms = None, image_transforms = None,):\n",
    "        self.audio_transforms = audio_transforms\n",
    "        self.img_transforms = image_transforms\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = copy.deepcopy(self.df.iloc[idx, :].values)\n",
    "        try:\n",
    "            wav = np.load(train_np_folder_path + sample[0] + \".npy\")\n",
    "        except:\n",
    "            wav, sr = librosa.load('../data/test/' + sample[0] + \".flac\", sr=None)\n",
    "            \n",
    "        tmin = float(sample[3]) * Config.SR\n",
    "        tmax = float(sample[5]) * Config.SR\n",
    "        center = np.round((tmin + tmax) / 2)\n",
    "        \n",
    "        multiplier = random.random() * 0.5 + 1\n",
    "        clip_size = (tmax - tmin) * multiplier\n",
    "        beginning = center - Config.LENGTH_2 / 2\n",
    "        if beginning < 0:\n",
    "            beginning = 0\n",
    "            \n",
    "        beginning = np.random.randint( beginning , center)\n",
    "        ending = beginning + Config.LENGTH_2\n",
    "        if ending > len(wav):\n",
    "            ending = len(wav)\n",
    "            beginning = ending - Config.LENGTH_2\n",
    "            \n",
    "        wav_slice = wav[int(beginning):int(ending)]\n",
    "        \n",
    "        beginning_time = beginning / Config.SR\n",
    "        ending_time = ending / Config.SR\n",
    "        recording_id = sample[0]\n",
    "        query_string = f\"recording_id == '{recording_id}' & \"\n",
    "        query_string += f\"t_min < {ending_time} & t_max > {beginning_time}\"\n",
    "        all_tp_events = self.df.query(query_string)\n",
    "\n",
    "        label_array = np.zeros(24, dtype=np.float32)\n",
    "        for species_id in all_tp_events[\"species_id\"].unique():\n",
    "            label_array[int(species_id)] = sample[-1]\n",
    "            if species_id == 12:\n",
    "                label_array[3] = 1\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "        #wav_slice = crop_or_pad(wav_slice)\n",
    "       \n",
    "        if self.audio_transforms: # and bird_id not in (3, 7, 8, 9):\n",
    "            #wav_slice =  self.audio_transforms(wav_slice)\n",
    "            wav_slice = self.audio_transforms(samples=wav_slice, sample_rate=Config.SR)\n",
    "            \n",
    "        \n",
    "        #new_sample_rate = 32000\n",
    "        #wav_slice = librosa.resample(wav_slice, Config.SR, new_sample_rate)\n",
    "            \n",
    "        #wav_slice = np.expand_dims(wav_slice, 0).astype(np.float32)\n",
    "        wav_slice = wav_slice.astype(np.float32) * 10.\n",
    "\n",
    "        return torch.tensor(wav_slice), label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "papermill": {
     "duration": 0.190203,
     "end_time": "2020-12-20T18:14:00.950678",
     "exception": false,
     "start_time": "2020-12-20T18:14:00.760475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1155 examples\n",
      "Validating on 218 examples\n"
     ]
    }
   ],
   "source": [
    "train_tp_folds[\"true\"] = 1\n",
    "train_fp_folds[\"true\"] = 0\n",
    "#X_train = train_tp_folds[(train_tp_folds['fold'] != Config.FOLD) & (train_tp_folds['fold'] != Config.TEST_FOLD)].reset_index(drop=True)\n",
    "X_train = train_tp_folds[(train_tp_folds['fold'] != Config.FOLD)].reset_index(drop=True)\n",
    "X_val = train_tp_folds[train_tp_folds['fold'] == Config.FOLD].reset_index(drop=True)\n",
    "#X_test = train_tp_folds[train_tp_folds['fold'] == Config.TEST_FOLD].reset_index(drop=True)\n",
    "#X_train = X_train[~(X_train.species_id == 12)]\n",
    "#X_val = X_val[~(X_val.species_id == 12)]\n",
    "\n",
    "\n",
    "\n",
    "_df = _df[_df.recording_id.isin(X_train.recording_id)]\n",
    "X_train = X_train[_df.columns]\n",
    "X_train = pd.concat([X_train, _df])\n",
    "X_train[\"label\"] = 1\n",
    "\n",
    "add_pseudo = False\n",
    "\n",
    "if add_pseudo:\n",
    "    X_train = X_train[pseudo_clear.columns]\n",
    "    X_train = pd.concat([X_train, pseudo_clear])\n",
    "\n",
    "print('Training on ' + str(len(X_train)) + ' examples')\n",
    "print('Validating on ' + str(len(X_val)) + ' examples')\n",
    "#print('Testing on ' + str(len(X_test)) + ' examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "papermill": {
     "duration": 0.031559,
     "end_time": "2020-12-20T18:14:01.049055",
     "exception": false,
     "start_time": "2020-12-20T18:14:01.017496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_transform_train = aa.Compose([\n",
    "  aa.OneOf([\n",
    "    aa.GaussianNoiseSNR(min_snr=5.0, max_snr=20.0),\n",
    "    aa.PinkNoiseSNR( min_snr=5.0, max_snr=20.0,)\n",
    "  ]),\n",
    "  aa.PitchShift(max_steps=4, sr=Config.SR, p=0.2),\n",
    "  #aa.TimeStretch(max_rate=1.2, p=0.1),\n",
    "  aa.TimeShift(sr=Config.SR),\n",
    "  aa.VolumeControl(mode=\"sine\", p=0.2 )\n",
    "])\n",
    "\n",
    "audio_transform = audioaa.Compose([\n",
    "    audioaa.AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "    #audioaa.AddShortNoises(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "    audioaa.AddGaussianSNR(min_SNR=0.001, max_SNR=1.0, p=0.5),\n",
    "    audioaa.PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "    #|audioaa.Shift(min_fraction=-0.5, max_fraction=0.5, p=0.5),\n",
    "    #audioaa.Normalize(),\n",
    "    #audioaa.PolarityInversion(p=0.5),\n",
    "    audioaa.Gain(min_gain_in_db=-12, max_gain_in_db=12, p=0.5),\n",
    "    #audioaa.ClippingDistortion(min_percentile_threshold=0, max_percentile_threshold=40, p=0.5)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n",
    "\n",
    "class PANNsLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bce = nn.BCELoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input_ = input[\"clipwise_output\"]\n",
    "        input_ = torch.where(torch.isnan(input_),\n",
    "                             torch.zeros_like(input_),\n",
    "                             input_)\n",
    "        input_ = torch.where(torch.isinf(input_),\n",
    "                             torch.zeros_like(input_),\n",
    "                             input_)\n",
    "\n",
    "        target = target.float()\n",
    "\n",
    "        return self.bce(input_, target)\n",
    "        \n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, use_coeffs = False, coeffs = None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.coeffs = coeffs\n",
    "        self.use_coeffs = use_coeffs\n",
    "\n",
    "    def forward(self, logit, target):\n",
    "        target = target.float()\n",
    "        batch_size = target.shape[0]\n",
    "        max_val = (-logit).clamp(min=0)\n",
    "        loss = logit - logit * target + max_val + \\\n",
    "            ((-max_val).exp() + (-logit - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-logit * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        \n",
    "        if self.use_coeffs:\n",
    "            loss = loss * self.coeffs.repeat(batch_size,1)\n",
    "        if len(loss.size()) == 2:\n",
    "            loss = loss.sum(dim=1)\n",
    "\n",
    "        return loss.mean()   \n",
    "\n",
    "class ImprovedPANNsLoss(nn.Module):\n",
    "    def __init__(self, output_key=\"logit\", weights=[1, 1], pos_weights =  None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_key = output_key\n",
    "        if output_key == \"logit\":\n",
    "            self.normal_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "        else:\n",
    "            self.normal_loss = nn.BCELoss()\n",
    "\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input_ = input[self.output_key]\n",
    "        target = target.float()\n",
    "\n",
    "        framewise_output = input[\"framewise_output\"]\n",
    "        clipwise_output_with_max, _ = framewise_output.max(dim=1)\n",
    "\n",
    "        normal_loss = self.normal_loss(input_, target)\n",
    "        auxiliary_loss = self.bce(clipwise_output_with_max, target)\n",
    "\n",
    "        return self.weights[0] * normal_loss + self.weights[1] * auxiliary_loss\n",
    "    \n",
    "class ImprovedFocalLoss(nn.Module):\n",
    "    def __init__(self, weights=[1, 1], use_coeffs = False, coeffs = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.focal = FocalLoss(coeffs=coeffs)\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input_ = input[\"logit\"]\n",
    "        target = target.float()\n",
    "\n",
    "        framewise_output = input[\"framewise_logit\"]\n",
    "        clipwise_output_with_max, _ = framewise_output.max(dim=1)\n",
    "\n",
    "        normal_loss = self.focal(input_, target)\n",
    "        auxiliary_loss = self.focal(clipwise_output_with_max, target)\n",
    "\n",
    "        return self.weights[0] * normal_loss + self.weights[1] * auxiliary_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AudioSEDModel(**model_param)\n",
    "#model.load_state_dict(torch.load('best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "papermill": {
     "duration": 10.752926,
     "end_time": "2020-12-20T18:14:11.820042",
     "exception": false,
     "start_time": "2020-12-20T18:14:01.067116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = RainforestDataset(X_train, audio_transforms=audio_transform, image_transforms=None)\n",
    "train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers = Config.NUM_WORKERS, drop_last = False)\n",
    "\n",
    "coeffiicients = np.array([0.25, 0.289, 0.238, 0.508, 0.229, 0.221, 0.212, \n",
    "                          0.285, 0.228, 0.215, 0.218, 0.263, 0.297, 0.216, \n",
    "                          0.247, 0.279, 0.220, 0.218, 0.360, 0.212, 0.215, \n",
    "                          0.221,0.219, 0.240])\n",
    "\n",
    "pos_weights = torch.ones(Config.NUM_BIRDS).cuda()\n",
    "pos_weights[3] = 5\n",
    "pos_weights[8] = 3\n",
    "\n",
    "criterion = ImprovedPANNsLoss(pos_weights=pos_weights)\n",
    "criterion_focal = ImprovedFocalLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=Config.LR_ADAM, weight_decay = 0.01)# momentum = 0.9)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=Config.LR, weight_decay=Config.WEIGHT_DECAY, momentum=Config.MOMENTUM)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma = 0.8)\n",
    "#scheduler =torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=500, T_mult=1, eta_min=1e-6)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 2, factor = 0.7, mode = \"max\")\n",
    "scheduler =torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n",
    "mixer = BatchMixer(p=0.5)\n",
    "mixup_augmenter = Mixup(mixup_alpha=1.)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    loss_function = loss_function.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_val_file(record_id, df):\n",
    "\n",
    "    wav = np.load('../data/train_np/' + record_id + \".npy\")\n",
    "        # Split for enough segments to not miss anything\n",
    "        #segments = len(wav) / Config.LENGTH_1\n",
    "        #segments = int(np.ceil(segments))\n",
    "    window = 10 * Config.SR\n",
    "    #stride = 5 * Config.SR\n",
    "    full_length = 60 * Config.SR\n",
    "\n",
    "    mel_array = []\n",
    "    #for i in range(0, full_length + stride - window, stride):\n",
    "    for i in range(0, full_length, window):\n",
    "        \n",
    "            wav_slice = wav[i:i+window]\n",
    "            #new_sample_rate = 32000\n",
    "            #wav_slice = librosa.resample(wav_slice, Config.SR, new_sample_rate)\n",
    "            #wav_slice = np.expand_dims(wav_slice, axis=0).astype(np.float32) \n",
    "            wav_slice = wav_slice.astype(np.float32) * 10.\n",
    "            mel_array.append(wav_slice)\n",
    "        \n",
    "        \n",
    "    val_labels_array = np.zeros(Config.NUM_BIRDS, dtype=np.single)\n",
    "    species_ids = copy.deepcopy(df[(df.recording_id==record_id)].species_id.unique())\n",
    "    val_labels_array[species_ids] = 1.\n",
    "    if 12 in species_ids:\n",
    "        val_labels_array[3] = 1.\n",
    "        \n",
    "    \n",
    "    return np.array(mel_array), val_labels_array\n",
    "\n",
    "def lwlrap(truth, scores):\n",
    "    \"\"\"Calculate the overall lwlrap using sklearn.metrics.lrap.\"\"\"\n",
    "    # sklearn doesn't correctly apply weighting to samples with no labels, so just skip them.\n",
    "    sample_weight = np.sum(truth > 0, axis=1)\n",
    "    nonzero_weight_sample_indices = np.flatnonzero(sample_weight > 0)\n",
    "    overall_lwlrap = label_ranking_average_precision_score(\n",
    "      truth[nonzero_weight_sample_indices, :] > 0,\n",
    "      scores[nonzero_weight_sample_indices, :],\n",
    "      sample_weight=sample_weight[nonzero_weight_sample_indices])\n",
    "    return overall_lwlrap\n",
    "\n",
    "\n",
    "def validate(model, files_ids, df):\n",
    "        val_loss = []\n",
    "        val_corr = []\n",
    "        val_metrics = []\n",
    "        model.eval()\n",
    "        for i in tqdm(range(0, len(files_ids))):\n",
    "            data, target = load_val_file(files_ids[i], X_val)\n",
    "            data, target = torch.tensor(data), torch.tensor(target)\n",
    "            data = data.float()\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.cuda(), target.cuda().unsqueeze(0)\n",
    "            output = model(data)\n",
    "            framewise_output = output[\"framewise_output\"]\n",
    "            output, _ = framewise_output.max(dim=1) \n",
    "            output, _ = torch.max(output, 0)\n",
    "          \n",
    "            output = output.unsqueeze(0)\n",
    "            #print(output.shape)\n",
    "            loss = loss_function(output, target)\n",
    "            #loss = lsep_loss(output, target)\n",
    "            #loss = criterion(output, target)\n",
    "            #loss = 0\n",
    "            \n",
    "            val_metric = lwlrap(target.cpu().detach().numpy(), output.cpu().detach().numpy())\n",
    "            vals, answers = torch.max(output, 1)\n",
    "            vals, targets = torch.max(target, 1)\n",
    "            val_metrics.append(val_metric.item())\n",
    "            corrects = 0\n",
    "            for i in range(0, len(answers)):\n",
    "                if answers[i] == targets[i]:\n",
    "                    corrects = corrects + 1\n",
    "            val_corr.append(corrects)\n",
    "            val_loss.append(loss.item())\n",
    "        valid_epoch_metric = sum(val_metrics) / len(val_loss)\n",
    "\n",
    "        return val_loss, val_corr, valid_epoch_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_corrects = 0\n",
    "files_ids = copy.deepcopy(X_val.recording_id.unique())\n",
    "mixup=False\n",
    "# Train loop\n",
    "print('Starting training loop')\n",
    "for e in range(0, 200):\n",
    "    # Stats\n",
    "    train_loss = []\n",
    "    train_corr = []\n",
    "    \n",
    "    # Single epoch - train\n",
    "    model.train()\n",
    "    for batch, (data, target) in tqdm(enumerate(train_loader)):\n",
    "        data = data.float()\n",
    "        if mixup:\n",
    "            mixup_lambda = torch.tensor(mixup_augmenter.get_lambda(len(data)))\n",
    "            target = do_mixup(target, mixup_lambda)\n",
    "        #data, target = mixer(data, target)\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            if mixup:\n",
    "                mixup_lambda =  mixup_lambda.cuda()\n",
    "\n",
    "        #print(data.shape, target.shape, mixup_lambda.shape)    \n",
    "        optimizer.zero_grad()\n",
    "        if mixup:\n",
    "            output = model(data, mixup_lambda )\n",
    "        else:\n",
    "            output = model(data)\n",
    "        #loss = loss_function(output, target)\n",
    "        #label_smoothing_list = [0.002, 0.0015, ]\n",
    "        #label_smoothing = random.choice(label_smoothing_list) \n",
    "        #targets_smooth = target * (1 - label_smoothing) + 0.5 * label_smoothing\n",
    "        \n",
    "        #output = output[\"logit\"]\n",
    "        #loss = loss_function(output, targets_smooth)\n",
    "        #loss = lsep_loss(output, target)\n",
    "        loss = criterion_focal(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "\n",
    "        # Stats\n",
    "        vals, answers = torch.max(output[\"clipwise_output\"], 1)\n",
    "        vals, targets = torch.max(target, 1)\n",
    "        corrects = 0\n",
    "        for i in range(0, len(answers)):\n",
    "            if answers[i] == targets[i]:\n",
    "                corrects = corrects + 1\n",
    "        train_corr.append(corrects)\n",
    "        train_loss.append(loss.item())\n",
    "    \n",
    "    # Stats\n",
    "    for g in optimizer.param_groups:\n",
    "        lr = g['lr']\n",
    "    print('Epoch ' + str(e) + ' training end. LR: ' + str(lr) + ', Loss: ' + str(sum(train_loss) / len(train_loss)) +\n",
    "          ', Correct answers: ' + str(sum(train_corr)) + '/' + str(train_dataset.__len__()))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "                # Stats\n",
    "        val_loss, val_corr, valid_epoch_metric = validate(model, files_ids, X_val)\n",
    "    # Stats\n",
    "    print('Epoch ' + str(e) + ' validation end. LR: ' + str(lr) + ', Loss: ' + str(sum(val_loss) / len(val_loss)) +\n",
    "          ', Correct answers: ' + str(sum(val_corr)) + '/' + str(len(files_ids)) + \", Val metric: \" + str(valid_epoch_metric))\n",
    "    \n",
    "    # If this epoch is better than previous on validation, save model\n",
    "    # Validation loss is the more common metric, but in this case our loss is misaligned with competition metric, making accuracy a better metric\n",
    "    if valid_epoch_metric > best_corrects:\n",
    "        print('Saving new best model at epoch ') #+ str(e) + ' ' + str(sum(val_corr)) + '/' + str(len(files_ids)))\n",
    "        torch.save(model.state_dict(), 'best_model_.pt')\n",
    "        best_corrects = valid_epoch_metric\n",
    "        \n",
    "    # Call every epoch\n",
    "    #scheduler.step(valid_epoch_metric)\n",
    "    scheduler.step()\n",
    "\n",
    "# Free memory\n",
    "#del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [00:21<00:00,  9.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.690632645853885, Correct answers: 127/197, Val metric: 0.8073301909596327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_model_.pt\"))\n",
    "files_ids = X_val.recording_id.unique()\n",
    "model.cuda()\n",
    "with torch.no_grad():\n",
    "                # Stats\n",
    "        answers_list =  []\n",
    "        targets_list = []\n",
    "        val_loss = []\n",
    "        val_corr = []\n",
    "        val_metrics = []\n",
    "        model.eval()\n",
    "        for i in tqdm(range(0, len(files_ids))):\n",
    "            data, target = load_val_file(files_ids[i], X_val)\n",
    "            data, target = torch.tensor(data), torch.tensor(target)\n",
    "            data = data.float()\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.cuda(), target.cuda().unsqueeze(0)\n",
    "            output = model(data)\n",
    "            #output = output.squeeze()\n",
    "            framewise_output = output[\"framewise_output\"]\n",
    "            output, _ = framewise_output.max(dim=1) \n",
    "            output, _ = torch.max(output, 0)\n",
    "            #output, _ = torch.max(output[\"clipwise_output\"], 0)\n",
    "            output = output.unsqueeze(0)\n",
    "            #print(output.shape)\n",
    "            loss = lsep_loss(output, target)\n",
    "            val_metric = lwlrap(target.cpu().numpy(), output.cpu().numpy())\n",
    "            vals, answers = torch.max(output, 1)\n",
    "            vals, targets = torch.max(target, 1)\n",
    "            answers_list.append(answers.item())\n",
    "            targets_list.append(targets.item())\n",
    "            val_metrics.append(val_metric.item())\n",
    "            corrects = 0\n",
    "            for i in range(0, len(answers)):\n",
    "                if answers[i] == targets[i]:\n",
    "                    corrects = corrects + 1\n",
    "            val_corr.append(corrects)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    \n",
    "valid_epoch_metric = sum(val_metrics) / len(val_loss)\n",
    "# Stats\n",
    "print('Loss: ' + str(sum(val_loss) / len(val_loss)) +\n",
    "      ', Correct answers: ' + str(sum(val_corr)) + '/' + str(len(files_ids)) + \", Val metric: \" + str(valid_epoch_metric))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 6), (1, 2), (2, 2), (3, 19), (4, 4), (5, 1), (7, 5), (8, 4), (9, 1), (10, 1), (11, 5), (13, 2), (14, 2), (15, 6), (16, 2), (17, 1), (20, 2), (21, 4), (23, 1)]\n",
      "[(0, 9), (1, 9), (2, 5), (3, 27), (4, 9), (5, 9), (6, 8), (7, 9), (8, 9), (9, 5), (10, 8), (11, 9), (13, 9), (14, 8), (15, 8), (16, 7), (17, 7), (19, 7), (20, 6), (21, 8), (22, 7), (23, 14)]\n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "for i in range(len(answers_list)):\n",
    "    if answers_list[i] != targets_list[i]:\n",
    "        errors.append(targets_list[i])\n",
    "        \n",
    "from collections import Counter\n",
    "error_count = sorted(Counter(errors).items(),key = lambda i: i[0])\n",
    "target_count = sorted(Counter(targets_list).items(),key = lambda i: i[0])\n",
    "print(error_count, target_count, sep = \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.995151,
     "end_time": "2020-12-20T18:27:34.774887",
     "exception": false,
     "start_time": "2020-12-20T18:27:33.779736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Already defined above; for reference\n",
    "fft = 2048\n",
    "hop = 512 * 1\n",
    "# Less rounding errors this way\n",
    "sr = 48000\n",
    "length = 20 * sr\n",
    "fmin = 84\n",
    "fmax = 15056\n",
    "\n",
    "\n",
    "def load_test_file(f): \n",
    "    wav, sr = librosa.load('../data/test/' + f, sr=None)\n",
    "\n",
    "        # Split for enough segments to not miss anything\n",
    "    segments = len(wav) / length\n",
    "    segments = int(np.ceil(segments))\n",
    "    \n",
    "    mel_array = []\n",
    "    \n",
    "    for i in range(0, segments):\n",
    "        # Last segment going from the end\n",
    "        if (i + 1) * length > len(wav):\n",
    "            wav_slice = wav[len(wav) - length:len(wav)]\n",
    "        else:\n",
    "            wav_slice = wav[i * length:(i + 1) * length]\n",
    "        #new_sample_rate = 24000\n",
    "        #wav_slice = librosa.resample(slice, Config.SR, new_sample_rate)\n",
    "        #wav_slice = np.expand_dims(wav_slice, axis=0).astype(np.float32) \n",
    "        wav_slice = wav_slice.astype(np.float32) \n",
    "        mel_array.append(wav_slice)\n",
    "    \n",
    "    return np.array(mel_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AudioSEDModel(**model_param)\n",
    "model.load_state_dict(torch.load(f\"best_model_sed_0.8.pt\"))\n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "PERIOD = 20\n",
    "global_time = 0.0\n",
    "threshold = 0.1\n",
    "estimated_event_list = []\n",
    "\n",
    "\n",
    "for i in range(0, len(test_files)):\n",
    "    global_time = 0.0\n",
    "    data = load_test_file(test_files[i])\n",
    "    file_id = str.split(test_files[i], '.')[0]\n",
    "    for part in data:\n",
    "    \n",
    "        part = torch.tensor(part).unsqueeze(0)\n",
    "        part = part.float()\n",
    "        if torch.cuda.is_available():\n",
    "            part = part.cuda()\n",
    "\n",
    "        output = model(part)\n",
    "\n",
    "        framewise_outputs = output[\"framewise_output\"].detach().cpu().numpy()[0]\n",
    "\n",
    "        thresholded = framewise_outputs >= threshold\n",
    "\n",
    "        #print(thresholded)\n",
    "        #print(thresholded.shape)\n",
    "\n",
    "        for target_idx in range(thresholded.shape[1]):\n",
    "            if thresholded[:, target_idx].mean() == 0:\n",
    "                pass\n",
    "            else:\n",
    "                detected = np.argwhere(thresholded[:, target_idx]).reshape(-1)\n",
    "                head_idx = 0\n",
    "                tail_idx = 0\n",
    "                while True:\n",
    "                    if (tail_idx + 1 == len(detected)) or (detected[tail_idx + 1] - detected[tail_idx] != 1):\n",
    "                        onset = 0.01 * detected[head_idx] + global_time\n",
    "                        offset = 0.01 * detected[tail_idx] + global_time\n",
    "                        onset_idx = detected[head_idx]\n",
    "                        offset_idx = detected[tail_idx]\n",
    "                        max_confidence = framewise_outputs[onset_idx:offset_idx, target_idx].max()\n",
    "                        mean_confidence = framewise_outputs[onset_idx:offset_idx, target_idx].mean()\n",
    "                        estimated_event = {\n",
    "                            \"file_id\": file_id,\n",
    "                            \"species_id\": target_idx,\n",
    "                            \"onset\": onset,\n",
    "                            \"offset\": offset,\n",
    "                            \"max_confidence\": max_confidence,\n",
    "                            \"mean_confidence\": mean_confidence\n",
    "                        }\n",
    "                        estimated_event_list.append(estimated_event)\n",
    "                        head_idx = tail_idx + 1\n",
    "                        tail_idx = tail_idx + 1\n",
    "                        if head_idx >= len(detected):\n",
    "                            break\n",
    "                    else:\n",
    "                        tail_idx += 1\n",
    "\n",
    "        global_time += PERIOD\n",
    "\n",
    "prediction_df = pd.DataFrame(estimated_event_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1992"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction_df.file_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.to_csv(\"pseudolabels_raw_sed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(columns=['recording_id','s0','s1','s2','s3','s4','s5','s6','s7','s8','s9','s10','s11',\n",
    "                               's12','s13','s14','s15','s16','s17','s18','s19','s20','s21','s22','s23'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.read_csv(\"pseudolabels_raw_sed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1992"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction_df.file_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction_df = pd.read_csv(\"pseudolabels_raw_fold1.csv\")\n",
    "for file_id, sub_df in prediction_df.groupby(\"file_id\"):\n",
    "    events = sub_df[[\"file_id\", \"species_id\", \"onset\", \"offset\", \"max_confidence\", ]]\n",
    "    sub_row = []\n",
    "    recording_id = events.file_id.unique()[0]\n",
    "    sub_row.append(recording_id)\n",
    "    unique = events.species_id.unique()\n",
    "    label_array = np.zeros(24, dtype=np.float32)\n",
    "    for i in unique:\n",
    "        pred_proba = events[events.species_id==i].max_confidence.max()\n",
    "\n",
    "        label_array[int(i)] = pred_proba \n",
    "    sub_row.extend(list(label_array))\n",
    "    sub_series = pd.Series(sub_row, index = submission.columns)\n",
    "    submission = submission.append(sub_series, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1992, 25)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"test_submission_from_frames.csv\", index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "papermill": {
   "duration": 1833.679699,
   "end_time": "2020-12-20T18:44:03.770668",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-20T18:13:30.090969",
   "version": "2.1.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0f15442e94a44248b50672c2777030c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4c435759345342aebeb8f5670af57f75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e56ad2e6997c43d695706f19b3c85f92",
       "placeholder": "​",
       "style": "IPY_MODEL_0f15442e94a44248b50672c2777030c9",
       "value": " 105M/105M [00:05&lt;00:00, 22.0MB/s]"
      }
     },
     "8b8ab01ab9604583ae4dbf53f3058ecf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e0f16c2b57ec45a0a100f1e728ab3d50",
       "max": 110273258,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_98b001f4de7143579e81c44054c3e04c",
       "value": 110273258
      }
     },
     "98b001f4de7143579e81c44054c3e04c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "c71fd161194b4e9d8f6df0c22ddc66c2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e0f16c2b57ec45a0a100f1e728ab3d50": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e56ad2e6997c43d695706f19b3c85f92": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ff6567e773d44b348f8b0ef7a4cf61e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8b8ab01ab9604583ae4dbf53f3058ecf",
        "IPY_MODEL_4c435759345342aebeb8f5670af57f75"
       ],
       "layout": "IPY_MODEL_c71fd161194b4e9d8f6df0c22ddc66c2"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
